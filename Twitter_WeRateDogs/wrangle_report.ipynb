{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The dataset that is wrangled, analyzed and visualized for this project  is the tweet archive of Twitter user @dog_rates with username WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs and adds a lighthearted comment.\n",
    "\n",
    " I have collected the data related to the **WeRateDogs** in a number of formats and from sources using Python and its libraries, evaluated its quality, and then cleaned it. The mentioned method is known as data wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps taken to wrangle the WeRateDogs dataset are as follows:\n",
    "\n",
    "* Step 1: Gathering data\n",
    "\n",
    "* Step 2: Assessing data\n",
    "\n",
    "* Step 3: Cleaning data\n",
    "\n",
    "* Step 4: Storing data\n",
    "\n",
    "* Step 5: Analyzing data\n",
    "\n",
    "* Step 6: Visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this project, I gathered the following three datasets:\n",
    "1. **Enhanced Twitter Archive**\n",
    "This dataset gathered and provided by Udacity contains basic tweet data for 2356 of WeRateDogs tweets. This dataset was downloaded directly from resources provided by Udacity and loaded into a dataframe with read_csv(). The dataset contains 17 columns, some of which are:\n",
    "  > * **tweet_id** : the unique identifier of a tweet.\n",
    "  > * **source** : where the tweet was uploaded\n",
    "  > * **timestamp** : the exact date and time the tweet was uploaded\n",
    "  > * **text** : comments in the tweet\n",
    "  > * **ratings** : dog rating coined from text\n",
    "  > * **name** : name of dog in the tweet\n",
    "   \n",
    "\n",
    "2. **Additional Data through the Twitter API**\n",
    "This dataset contains the number of tweet like and number of retweet of each tweet, since each tweet has a unique tweet id. This data should be scraped from the twitter API(tweepy), however for some reason, I have not been granted access to use the API. Udacity provided a url for this file that should have been gathered from tweepy. Using Python's **requests** library, I was able to get the content of the url loaded to a file and read the content into a dataframe. The dataset has 3 columns:\n",
    " > * **tweet_id** : the unique identifier of a tweet.\n",
    " > * **retweet_count** : number of retweet the post or tweet got\n",
    " > * **favorite_count** : number of likes the tweet got\n",
    " \n",
    " \n",
    "3. **Image Predictions File**\n",
    "The url for this file was provided by Udacity, I just needed to get the content of the file using the **requests** library. An instructor ran each and every image in the WeRateDogs Twitter archive through a neural network that can classify breeds of dogs to get these additional columns: \n",
    " > * **tweet_id** : the unique identifier of a tweet.\n",
    " > * **img_num** : number of images posted along with the comment, maximum is 4\n",
    " > * **p1** : neural network's first prediction for the image in the tweet\n",
    " > * **p1_conf** : how confident the algorithm is in its first prediction\n",
    " > * **p1_dog** : is the first prediction a breed of dog\n",
    " > * **p2** : neural network's second prediction for the image in the tweet\n",
    " > * **p2_conf** : how confident the algorithm is in its second prediction\n",
    " > * **p2_dog** : is the first prediction a breed of dog\n",
    " > * **p3** : neural network's third prediction for the image in the tweet\n",
    " > * **p3_conf** : how confident the algorithm is in its third prediction\n",
    " > * **p3_dog** : is the first prediction a breed of dog\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I gathered all three pieces of data, I then assess them visually and programmatically(using pandas functions like sample(), info(), describe(), duplicated() e.t.c) for quality and tidiness issues.\n",
    "\n",
    "**Data Quality**: I checked for low-quality data, which has problems with its content, might include things like erroneous data, corrupted data, and duplicate data. The data quality issues I discovered are itemized below:\n",
    "1. twitter-archive-enhanced table: Source column is not in a proper format\n",
    "\n",
    "2. twitter-archive-enhanced table: Columns that are not relevant to analysis are present\n",
    "\n",
    "3. twitter-archive-enhanced table: incorrect datatype for timestamp column\n",
    "\n",
    "4. image-prediction table: missing records leading to incomplete dataset (2075 out of 2356)\n",
    "\n",
    "5. twitter-archive-enhanced table: Null values represented as None in name column\n",
    "\n",
    "6. image-prediction table: some values in P1, P2 ans p3 columns begin with lowercase\n",
    "\n",
    "7. twitter-archive-enhanced table: maximum and minimum value for rating_denominator column are 170 and 0 instead of 10\n",
    "\n",
    "8. image-preiction table: p1, p2 and p3 column values containing underscore(_)\n",
    "\n",
    "**Data Tidiness**: I assessed data for structural issues that will slow down the cleaning and analyzing process. The data tidiness issues I found are itemized below:\n",
    "1. twitter-archive-enhanced table: Dog stages are divided into columns \n",
    "\n",
    "2. image-preiction table: names of columns are not explanatory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began with creating a copy of each dataframe and then cleaning the pieces of data one after the other based on the quality and tidiness issues in each of the three datasets and finally testing each codes one after the other to see if the issues have been properly fixed.\n",
    "\n",
    "After the thorough cleaning process, I merged the three dataset into one main dataset that contains all the column that are necessary for analysis. The columns in the merged datasets are listed below:\n",
    "\n",
    " > * **tweet_id** : the unique identifier of a tweet.\n",
    " > * **source** : where the tweet was uploaded\n",
    " > * **timestamp** : the exact date and time the tweet was uploaded\n",
    " > * **text** : comments in the tweet\n",
    " > * **ratings** : dog ratings coined from text\n",
    " > * **name** : name of dog in the tweet\n",
    " > * **dog_stage** : age or size of dog in category\n",
    " > * **img_num** : number of images posted along with the comment, maximum is 4\n",
    " > * **first_prediction** : neural network's first prediction for the image in the tweet\n",
    " > * **first_pred_score** : how confident the algorithm is in its first prediction\n",
    " > * **first_is_dog** : is the first prediction a breed of dog\n",
    " > * **second_prediction** : neural network's second prediction for the image in the tweet\n",
    " > * **second_pred_score** : how confident the algorithm is in its second prediction\n",
    " > * **second_is_dog** : is the first prediction a breed of dog\n",
    " > * **third_prediction** : neural network's third prediction for the image in the tweet\n",
    " > * **third_pred_score** : how confident the algorithm is in its third prediction\n",
    " > * **third_is_dog** : is the first prediction a breed of dog\n",
    " > * **retweet_count** : number of retweet the post or tweet got\n",
    " > * **favorite_count** : number of likes the tweet got\n",
    " > * **year** : year tweet was posted\n",
    " > * **month** : month tweet was posted\n",
    " > * **day** : day tweet was posted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning and merging the datasets, I stored the cleaned dataframe in a CSV file named **twitter_archive_master.csv**. This cleaned dataframe served as the one used for performing analysis and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis was performed on the cleaned data with python and its library, pandas to create insights on the WeRateDogs twitter archive. Some of the insights created are:\n",
    "\n",
    "1. The Dog breed with highest total rating, total likes and total retweet count on the WeRateDogs profile is Golden Retriever. From the analysis of the dataset, it is concluded that Golden Retrievers are one of the most popular breed of dogs, followed by Labrador Retriever and Pembroke. This could be due to the fact that they are hardworking, great therapy dogs and they are the cutest, in my opinion.\n",
    " \n",
    "2. The first dog to ever feature on the WeRateDogs profile according to the gathered dataset is a Welsh Springer Spaniel, thsis dog was introduced to us on 15th of November 2015, although no further information like its name and dog stage was provided in the dataset but we know he or she is a good dog with an amazing rating of 8 out of 10.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The insights made from the analysis were finally visualized with matplotlib library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Only 25% of the dog_stage values were available, this limited the correctness on the analysis I made on popular dog stage.\n",
    "\n",
    "2. The predicted dog breed had some columns where all three predictions were not a dog breed. Some analysis of dog breeds were affected by this incorrectness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling can be a long and iterative process but after making myself familiar with each and every column in the dataset, I got comfortable with assessing, cleaning and enriching the dataset. Using Python's libraries mae the wrangling process easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
